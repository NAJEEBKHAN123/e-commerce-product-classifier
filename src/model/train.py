# src/model/train.py - UPDATED FOR COLAB
import torch
from torch.utils.data import DataLoader
import os
import time
import numpy as np

class Trainer:
    """Training class for model training"""
    
    def __init__(self, batch_size, learning_rate, data_loader, model, 
                 model_path, device, class_weights=None):
        self.batch_size = batch_size
        self.data_loader = data_loader
        self.model = model
        self.lr = learning_rate
        self.device = device
        
        # Use class weights for imbalance
        if class_weights is not None:
            if isinstance(class_weights, list):
                class_weights = torch.tensor(class_weights, dtype=torch.float32)
            class_weights = class_weights.to(device)
            self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)
        else:
            self.loss_fn = torch.nn.CrossEntropyLoss()
            
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        
        # FIXED: Removed verbose=True parameter for Colab compatibility
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5
        )
        
        self.model_dir = "models"
        os.makedirs(self.model_dir, exist_ok=True)
        self.model_path = model_path
        
        # Track training history
        self.train_loss_history = []
        self.train_acc_history = []
        self.current_lr = self.lr
    
    def start_training_loop(self, epoch):
        """Run training for one epoch"""
        try:
            self.model.train()  # Set model to training mode
            epoch_start_time = time.time()
            
            total_loss = 0
            correct = 0
            total = 0
            
            # Store batch losses for learning rate scheduler
            batch_losses = []
            
            for batch_idx, batch_data in enumerate(self.data_loader):
                batch_start_time = time.time()
                
                # Handle different data loader formats
                if len(batch_data) == 3:
                    images, labels, _ = batch_data
                elif len(batch_data) == 2:
                    images, labels = batch_data
                else:
                    print(f"‚ùå Unexpected batch format: {len(batch_data)} elements")
                    continue
                
                # Move data to device
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # Forward pass
                outputs = self.model(images)
                loss = self.loss_fn(outputs, labels)
                
                # Backward pass and optimize
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                # Calculate metrics
                total_loss += loss.item()
                batch_losses.append(loss.item())
                
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # Print progress
                if batch_idx % 10 == 0:
                    batch_time = time.time() - batch_start_time
                    current_lr = self.optimizer.param_groups[0]['lr']
                    print(f"Epoch {epoch}, Batch {batch_idx}/{len(self.data_loader)}: "
                          f"Loss = {loss.item():.4f}, LR = {current_lr:.6f}, Time = {batch_time:.2f}s")
            
            # Calculate epoch metrics
            avg_loss = total_loss / len(self.data_loader)
            accuracy = 100.0 * correct / total
            epoch_time = time.time() - epoch_start_time
            
            # Update learning rate scheduler
            self.scheduler.step(avg_loss)
            self.current_lr = self.optimizer.param_groups[0]['lr']
            
            # Store history
            self.train_loss_history.append(avg_loss)
            self.train_acc_history.append(accuracy)
            
            print(f"\nEpoch {epoch} Summary:")
            print(f"  Training Loss: {avg_loss:.4f}")
            print(f"  Training Accuracy: {accuracy:.2f}%")
            print(f"  Learning Rate: {self.current_lr:.6f}")
            print(f"  Epoch Time: {epoch_time:.2f} seconds")
            print(f"  Correct: {correct}/{total}")
            
            return avg_loss, accuracy
            
        except Exception as e:
            print(f"Error in training loop at epoch {epoch}: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def save_model(self, epoch=None, accuracy=None):
        """Save model checkpoint"""
        try:
            # Create models directory if it doesn't exist
            os.makedirs(self.model_dir, exist_ok=True)
            
            if epoch is not None and accuracy is not None:
                filename = f"{self.model_path}_epoch{epoch}_acc{accuracy:.2f}.pth"
            else:
                filename = f"{self.model_path}.pth"
            
            final_path = os.path.join(self.model_dir, filename)
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'loss': self.train_loss_history[-1] if self.train_loss_history else 0,
                'accuracy': self.train_acc_history[-1] if self.train_acc_history else 0,
                'train_loss_history': self.train_loss_history,
                'train_acc_history': self.train_acc_history,
                'learning_rate': self.current_lr,
                'config': {
                    'batch_size': self.batch_size,
                    'initial_lr': self.lr,
                    'device': str(self.device)
                }
            }, final_path)
            
            print(f"Model saved to: {final_path}")
            return final_path
            
        except Exception as e:
            print(f"Error saving model: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load_model(self, model_path):
        """Load model checkpoint"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            if 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            if 'train_loss_history' in checkpoint:
                self.train_loss_history = checkpoint['train_loss_history']
            if 'train_acc_history' in checkpoint:
                self.train_acc_history = checkpoint['train_acc_history']
            
            if 'learning_rate' in checkpoint:
                self.current_lr = checkpoint['learning_rate']
            
            print(f"Model loaded from: {model_path}")
            print(f"Previous training: {len(self.train_loss_history)} epochs")
            print(f"Best loss: {min(self.train_loss_history) if self.train_loss_history else 'N/A':.4f}")
            print(f"Best accuracy: {max(self.train_acc_history) if self.train_acc_history else 'N/A':.2f}%")
            
            return checkpoint.get('epoch', 0)
            
        except Exception as e:
            print(f"Error loading model: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def get_training_history(self):
        """Get training history for plotting"""
        return {
            'loss': self.train_loss_history,
            'accuracy': self.train_acc_history,
            'learning_rate': self.current_lr
        }
    
    def get_current_metrics(self):
        """Get current training metrics"""
        if self.train_loss_history and self.train_acc_history:
            return {
                'loss': self.train_loss_history[-1],
                'accuracy': self.train_acc_history[-1],
                'learning_rate': self.current_lr
            }
        return None